@article{hinton2015distilling,
  title   = {Distilling the Knowledge in a Neural Network},
  author  = {Geoffrey E. Hinton and Oriol Vinyals and Jeffrey Dean},
  journal = {ArXiv},
  year    = {2015},
  volume  = {abs/1503.02531}
}

@inproceedings{Vaswani2017AttentionIA,
    title     = {Attention is All you Need},
    author    = {Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    booktitle = {NIPS},
    year      = {2017}
}

@article{Tschandl_2018,
   title={The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions},
   volume={5},
   ISSN={2052-4463},
   url={http://dx.doi.org/10.1038/sdata.2018.161},
   DOI={10.1038/sdata.2018.161},
   number={1},
   journal={Scientific Data},
   publisher={Springer Science and Business Media LLC},
   author={Tschandl, Philipp and Rosendahl, Cliff and Kittler, Harald},
   year={2018},
   month=aug }

@article{T5,
  author       = {Colin Raffel and
                  Noam Shazeer and
                  Adam Roberts and
                  Katherine Lee and
                  Sharan Narang and
                  Michael Matena and
                  Yanqi Zhou and
                  Wei Li and
                  Peter J. Liu},
  title        = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
                  Transformer},
  journal      = {CoRR},
  volume       = {abs/1910.10683},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.10683},
  eprinttype    = {arXiv},
  eprint       = {1910.10683},
  timestamp    = {Fri, 05 Feb 2021 15:43:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-10683.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}

@misc{deepseek,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

@misc{wandb,
title = {Experiment Tracking with Weights and Biases},
year = {2020},
note = {Software available from wandb.com},
url={https://www.wandb.com/},
author = {Biewald, Lukas},
}

@InProceedings{Zhai_2022_CVPR,
    author    = {Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
    title     = {Scaling Vision Transformers},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {12104-12113}
}

@inproceedings{jiang-etal-2024-med,
    title = "{M}ed-{M}o{E}: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models",
    author = "Jiang, Songtao  and
      Zheng, Tuo  and
      Zhang, Yan  and
      Jin, Yeying  and
      Yuan, Li  and
      Liu, Zuozhu",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.221/",
    doi = "10.18653/v1/2024.findings-emnlp.221",
    pages = "3843--3860",
    abstract = "Recent advancements in general-purpose or domain-specific multimodal large language models (LLMs) have witnessed remarkable progress for medical decision-making. However, they are designated for specific classification or generative tasks, and require model training or finetuning on large-scale datasets with sizeable parameters and tremendous computing, hindering their clinical utility across diverse resource-constrained scenarios in practice. In this paper, we propose a novel and lightweight framework Med-MoE (Mixture-of-Experts) that tackles both discriminative and generative multimodal medical tasks. The learning of Med-MoE consists of three steps: multimodal medical alignment, Instruction tuning and routing, and domain-specific MoE tuning. After aligning multimodal medical images with LLM tokens, we then enable the model for different multimodal medical tasks with instruction tuning, together with a trainable router tailored for expert selection across input modalities. Finally, the model is tuned by integrating the router with multiple domain-specific experts, which are selectively activated and further empowered by meta experts. Comprehensive experiments on both open- and close-end medical question answering (Med-VQA) and image classification tasks across datasets such as VQA-RAD, SLAKE and Path-VQA demonstrate that our model can achieve performance superior to or on par with state-of-the-art baselines, while only requiring approximately 30{\%}-50{\%} of activated model parameters. Extensive analysis and ablations corroborate the effectiveness and practical utility of our method."
}

@article{RAHMAN2021104319,
title = {Exploring the effect of image enhancement techniques on COVID-19 detection using chest X-ray images},
journal = {Computers in Biology and Medicine},
volume = {132},
pages = {104319},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104319},
url = {https://www.sciencedirect.com/science/article/pii/S001048252100113X},
author = {Tawsifur Rahman and Amith Khandakar and Yazan Qiblawey and Anas Tahir and Serkan Kiranyaz and Saad Bin {Abul Kashem} and Mohammad Tariqul Islam and Somaya {Al Maadeed} and Susu M. Zughaier and Muhammad Salman Khan and Muhammad E.H. Chowdhury},
keywords = {COVID-19, Image enhancement, Chest X-ray images, Convolutional neural networks, Lung segmentation},
abstract = {Computer-aided diagnosis for the reliable and fast detection of coronavirus disease (COVID-19) has become a necessity to prevent the spread of the virus during the pandemic to ease the burden on the healthcare system. Chest X-ray (CXR) imaging has several advantages over other imaging and detection techniques. Numerous works have been reported on COVID-19 detection from a smaller set of original X-ray images. However, the effect of image enhancement and lung segmentation of a large dataset in COVID-19 detection was not reported in the literature. We have compiled a large X-ray dataset (COVQU) consisting of 18,479 CXR images with 8851 normal, 6012 non-COVID lung infections, and 3616 COVID-19 CXR images and their corresponding ground truth lung masks. To the best of our knowledge, this is the largest public COVID positive database and the lung masks. Five different image enhancement techniques: histogram equalization (HE), contrast limited adaptive histogram equalization (CLAHE), image complement, gamma correction, and balance contrast enhancement technique (BCET) were used to investigate the effect of image enhancement techniques on COVID-19 detection. A novel U-Net model was proposed and compared with the standard U-Net model for lung segmentation. Six different pre-trained Convolutional Neural Networks (CNNs) (ResNet18, ResNet50, ResNet101, InceptionV3, DenseNet201, and ChexNet) and a shallow CNN model were investigated on the plain and segmented lung CXR images. The novel U-Net model showed an accuracy, Intersection over Union (IoU), and Dice coefficient of 98.63%, 94.3%, and 96.94%, respectively for lung segmentation. The gamma correction-based enhancement technique outperforms other techniques in detecting COVID-19 from the plain and the segmented lung CXR images. Classification performance from plain CXR images is slightly better than the segmented lung CXR images; however, the reliability of network performance is significantly improved for the segmented lung images, which was observed using the visualization technique. The accuracy, precision, sensitivity, F1-score, and specificity were 95.11%, 94.55%, 94.56%, 94.53%, and 95.59% respectively for the segmented lung images. The proposed approach with very reliable and comparable performance will boost the fast and robust COVID-19 detection using chest X-ray images.}
}

@ARTICLE{9144185,
  author={Chowdhury, Muhammad E. H. and Rahman, Tawsifur and Khandakar, Amith and Mazhar, Rashid and Kadir, Muhammad Abdul and Mahbub, Zaid Bin and Islam, Khandakar Reajul and Khan, Muhammad Salman and Iqbal, Atif and Emadi, Nasser Al and Reaz, Mamun Bin Ibne and Islam, Mohammad Tariqul},
  journal={IEEE Access}, 
  title={Can AI Help in Screening Viral and COVID-19 Pneumonia?}, 
  year={2020},
  volume={8},
  number={},
  pages={132665-132676},
  keywords={Diseases;Lung;Databases;X-ray imaging;Machine learning;Tools;COVID-19;Artificial intelligence;COVID-19 pneumonia;machine learning;transfer learning;viral pneumonia;computer-aided diagnostic tool},
  doi={10.1109/ACCESS.2020.3010287}}

@article{nih-xray,
  author       = {Xiaosong Wang and
                  Yifan Peng and
                  Le Lu and
                  Zhiyong Lu and
                  Mohammadhadi Bagheri and
                  Ronald M. Summers},
  title        = {ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on
                  Weakly-Supervised Classification and Localization of Common Thorax
                  Diseases},
  journal      = {CoRR},
  volume       = {abs/1705.02315},
  year         = {2017},
  url          = {http://arxiv.org/abs/1705.02315},
  eprinttype    = {arXiv},
  eprint       = {1705.02315},
  timestamp    = {Mon, 11 Nov 2024 14:54:08 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/WangPLLBS17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{yang2025mixtureexpertsintrinsicallyinterpretable,
      title={Mixture of Experts Made Intrinsically Interpretable}, 
      author={Xingyi Yang and Constantin Venhoff and Ashkan Khakzar and Christian Schroeder de Witt and Puneet K. Dokania and Adel Bibi and Philip Torr},
      year={2025},
      eprint={2503.07639},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.07639}, 
}

@article{Ismail2022InterpretableMO,
  title={Interpretable Mixture of Experts for Structured Data},
  author={Aya Abdelsalam Ismail and Sercan {\"O}. Arik and Jinsung Yoon and Ankur Taly and Soheil Feizi and Tomas Pfister},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.02107},
  url={https://api.semanticscholar.org/CorpusID:249394928}
}

@inproceedings{oikarinenlabel,
  title={Label-free Concept Bottleneck Models},
  author={Oikarinen, Tuomas and Das, Subhro and Nguyen, Lam M and Weng, Tsui-Wei},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@inproceedings{yang2023language,
  title={Language in a bottle: Language model guided concept bottlenecks for interpretable image classification},
  author={Yang, Yue and Panagopoulou, Artemis and Zhou, Shenghao and Jin, Daniel and Callison-Burch, Chris and Yatskar, Mark},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={19187--19197},
  year={2023}
}

@InProceedings{pmlr-v202-ghosh23c,
  title = 	 {Dividing and Conquering a {B}lack{B}ox to a Mixture of Interpretable Models: Route, Interpret, Repeat},
  author =       {Ghosh, Shantanu and Yu, Ke and Arabshahi, Forough and Batmanghelich, Kayhan},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {11360--11397},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/ghosh23c/ghosh23c.pdf},
  url = 	 {https://proceedings.mlr.press/v202/ghosh23c.html},
  abstract = 	 {ML model design either starts with an interpretable model or a Blackbox and explains it post hoc. Blackbox models are flexible but difficult to explain, while interpretable models are inherently explainable. Yet, interpretable models require extensive ML knowledge and tend to be less flexible, potentially underperforming than their Blackbox equivalents. This paper aims to blur the distinction between a post hoc explanation of a Blackbox and constructing interpretable models. Beginning with a Blackbox, we iteratively carve out a mixture of interpretable models and a residual network. The interpretable models identify a subset of samples and explain them using First Order Logic (FOL), providing basic reasoning on concepts from the Blackbox. We route the remaining samples through a flexible residual. We repeat the method on the residual network until all the interpretable models explain the desired proportion of data. Our extensive experiments show that our route, interpret, and repeat approach (1) identifies a richer diverse set of instance-specific concepts with high concept completeness via interpretable models by specializing in various subsets of data without compromising in performance, (2) identifies the relatively “harder” samples to explain via residuals, (3) outperforms the interpretable by-design models by significant margins during test-time interventions, (4) can be used to fix the shortcut learned by the original Blackbox.}
}

@misc{zhao2024hypermoebettermixtureexperts,
      title={HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts}, 
      author={Hao Zhao and Zihan Qiu and Huijia Wu and Zili Wang and Zhaofeng He and Jie Fu},
      year={2024},
      eprint={2402.12656},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.12656}, 
}

@misc{eigen2014learningfactoredrepresentationsdeep,
      title={Learning Factored Representations in a Deep Mixture of Experts}, 
      author={David Eigen and Marc'Aurelio Ranzato and Ilya Sutskever},
      year={2014},
      eprint={1312.4314},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1312.4314}, 
}


@InProceedings{pmlr-v119-koh20a,
  title = 	 {Concept Bottleneck Models},
  author =       {Koh, Pang Wei and Nguyen, Thao and Tang, Yew Siang and Mussmann, Stephen and Pierson, Emma and Kim, Been and Liang, Percy},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {5338--5348},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/koh20a/koh20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/koh20a.html},
  abstract = 	 {We seek to learn models that we can interact with using high-level concepts: if the model did not think there was a bone spur in the x-ray, would it still predict severe arthritis? State-of-the-art models today do not typically support the manipulation of concepts like "the existence of bone spurs", as they are trained end-to-end to go directly from raw input (e.g., pixels) to output (e.g., arthritis severity). We revisit the classic idea of first predicting concepts that are provided at training time, and then using these concepts to predict the label. By construction, we can intervene on these concept bottleneck models by editing their predicted concept values and propagating these changes to the final prediction. On x-ray grading and bird identification, concept bottleneck models achieve competitive accuracy with standard end-to-end models, while enabling interpretation in terms of high-level clinical concepts ("bone spurs") or bird attributes ("wing color"). These models also allow for richer human-model interaction: accuracy improves significantly if we can correct model mistakes on concepts at test time.}
}

@article{biomedclip,
author = {Sheng Zhang  and Yanbo Xu  and Naoto Usuyama  and Hanwen Xu  and Jaspreet Bagga  and Robert Tinn  and Sam Preston  and Rajesh Rao  and Mu Wei  and Naveen Valluri  and Cliff Wong  and Andrea Tupini  and Yu Wang  and Matt Mazzola  and Swadheen Shukla  and Lars Liden  and Jianfeng Gao  and Angela Crabtree  and Brian Piening  and Carlo Bifulco  and Matthew P. Lungren  and Tristan Naumann  and Sheng Wang  and Hoifung Poon },
title = {A Multimodal Biomedical Foundation Model Trained from Fifteen Million Image–Text Pairs},
journal = {NEJM AI},
volume = {2},
number = {1},
pages = {AIoa2400640},
year = {2025},
doi = {10.1056/AIoa2400640},

URL = {https://ai.nejm.org/doi/full/10.1056/AIoa2400640},
eprint = {https://ai.nejm.org/doi/pdf/10.1056/AIoa2400640}
,
    abstract = { BiomedCLIP is a fully open-access foundation model that achieves state-of-the-art performance on various biomedical tasks, paving the way for transformative multimodal biomedical discovery and applications. }
}

@inproceedings{CLIP,
  author       = {Alec Radford and
                  Jong Wook Kim and
                  Chris Hallacy and
                  Aditya Ramesh and
                  Gabriel Goh and
                  Sandhini Agarwal and
                  Girish Sastry and
                  Amanda Askell and
                  Pamela Mishkin and
                  Jack Clark and
                  Gretchen Krueger and
                  Ilya Sutskever},
  editor       = {Marina Meila and
                  Tong Zhang},
  title        = {Learning Transferable Visual Models From Natural Language Supervision},
  booktitle    = {Proceedings of the 38th International Conference on Machine Learning,
                  {ICML} 2021, 18-24 July 2021, Virtual Event},
  series       = {Proceedings of Machine Learning Research},
  volume       = {139},
  pages        = {8748--8763},
  publisher    = {{PMLR}},
  year         = {2021},
  url          = {http://proceedings.mlr.press/v139/radford21a.html},
  timestamp    = {Wed, 25 Aug 2021 17:11:17 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/RadfordKHRGASAM21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{blackbox,
title = {Medical artificial intelligence and the black box problem: a view based on the ethical principle of “do no harm”},
journal = {Intelligent Medicine},
volume = {4},
number = {1},
pages = {52-57},
year = {2024},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2023.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2667102623000578},
author = {Hanhui Xu and Kyle Michael James Shuttleworth},
keywords = {Medical artificial intelligence, Black box problem, Do no harm, Paternalism},
abstract = {One concern about the application of medical artificial intelligence (AI) regards the “black box” feature which can only be viewed in terms of its inputs and outputs, with no way to understand the AI's algorithm. This is problematic because patients, physicians, and even designers, do not understand why or how a treatment recommendation is produced by AI technologies. One view claims that the worry about black-box medicine is unreasonable because AI systems outperform human doctors in identifying the disease. Furthermore, under the medical AI-physician-patient model, the physician can undertake the responsibility of interpreting the medical AI's diagnosis. In this study, we focus on the potential harm caused by the unexplainability feature of medical AI and try to show that such possible harm is underestimated. We will seek to contribute to the literature from three aspects. First, we appealed to a thought experiment to show that although the medical AI systems perform better on accuracy, the harm caused by medical AI's misdiagnoses may be more serious than that caused by human doctors’ misdiagnoses in some cases. Second, in patient-centered medicine, physicians were obligated to provide adequate information to their patients in medical decision-making. However, the unexplainability feature of medical AI systems would limit the patient's autonomy. Last, we tried to illustrate the psychological and financial burdens that may be caused by the unexplainablity feature of medical AI systems, which seems to be ignored by the previous ethical discussions.}
}

@Article{Rudin2019,
author={Rudin, Cynthia},
title={Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
journal={Nature Machine Intelligence},
year={2019},
month={May},
day={01},
volume={1},
number={5},
pages={206-215},
abstract={Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision.},
issn={2522-5839},
doi={10.1038/s42256-019-0048-x},
url={https://doi.org/10.1038/s42256-019-0048-x}
}

@Article{Sitaula2021,
author={Sitaula, Chiranjibi
and Aryal, Sunil},
title={New bag of deep visual words based features to classify chest x-ray images for COVID-19 diagnosis},
journal={Health Information Science and Systems},
year={2021},
month={Jun},
day={18},
volume={9},
number={1},
pages={24},
abstract={Because the infection by Severe Acute Respiratory Syndrome Coronavirus 2 (COVID-19) causes the Pneumonia-like effect in the lung, the examination of Chest X-Rays (CXR) can help diagnose the disease. For automatic analysis of images, they are represented in machines by a set of semantic features. Deep Learning (DL) models are widely used to extract features from images. General deep features extracted from intermediate layers may not be appropriate to represent CXR images as they have a few semantic regions. Though the Bag of Visual Words (BoVW)-based features are shown to be more appropriate for different types of images, existing BoVW features may not capture enough information to differentiate COVID-19 infection from other Pneumonia-related infections.},
issn={2047-2501},
doi={10.1007/s13755-021-00152-w},
url={https://doi.org/10.1007/s13755-021-00152-w}
}


